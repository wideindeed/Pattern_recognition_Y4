{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 50.0,
  "eval_steps": 500,
  "global_step": 22350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.11185682326621924,
      "grad_norm": 474.13140869140625,
      "learning_rate": 1.9956152125279645e-05,
      "loss": 175.0443,
      "step": 50
    },
    {
      "epoch": 0.22371364653243847,
      "grad_norm": 205.83172607421875,
      "learning_rate": 1.9911409395973156e-05,
      "loss": 25.6649,
      "step": 100
    },
    {
      "epoch": 0.33557046979865773,
      "grad_norm": 95.84142303466797,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 11.1904,
      "step": 150
    },
    {
      "epoch": 0.44742729306487694,
      "grad_norm": 72.6605453491211,
      "learning_rate": 1.9821923937360182e-05,
      "loss": 6.9891,
      "step": 200
    },
    {
      "epoch": 0.5592841163310962,
      "grad_norm": 48.984249114990234,
      "learning_rate": 1.9777181208053693e-05,
      "loss": 4.6432,
      "step": 250
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 28.0860538482666,
      "learning_rate": 1.9732438478747204e-05,
      "loss": 3.0809,
      "step": 300
    },
    {
      "epoch": 0.7829977628635347,
      "grad_norm": 32.84109878540039,
      "learning_rate": 1.968769574944072e-05,
      "loss": 2.5481,
      "step": 350
    },
    {
      "epoch": 0.8948545861297539,
      "grad_norm": 45.81435012817383,
      "learning_rate": 1.964295302013423e-05,
      "loss": 2.005,
      "step": 400
    },
    {
      "epoch": 1.0067114093959733,
      "grad_norm": 29.266525268554688,
      "learning_rate": 1.959821029082774e-05,
      "loss": 1.7675,
      "step": 450
    },
    {
      "epoch": 1.1185682326621924,
      "grad_norm": 27.42583656311035,
      "learning_rate": 1.9553467561521256e-05,
      "loss": 1.5895,
      "step": 500
    },
    {
      "epoch": 1.2304250559284116,
      "grad_norm": 52.45271682739258,
      "learning_rate": 1.9508724832214767e-05,
      "loss": 1.4741,
      "step": 550
    },
    {
      "epoch": 1.342281879194631,
      "grad_norm": 132.49948120117188,
      "learning_rate": 1.946398210290828e-05,
      "loss": 1.3402,
      "step": 600
    },
    {
      "epoch": 1.45413870246085,
      "grad_norm": 27.263818740844727,
      "learning_rate": 1.941923937360179e-05,
      "loss": 1.2848,
      "step": 650
    },
    {
      "epoch": 1.5659955257270695,
      "grad_norm": 52.74026870727539,
      "learning_rate": 1.9374496644295304e-05,
      "loss": 1.1537,
      "step": 700
    },
    {
      "epoch": 1.6778523489932886,
      "grad_norm": 43.56673049926758,
      "learning_rate": 1.9329753914988816e-05,
      "loss": 1.1287,
      "step": 750
    },
    {
      "epoch": 1.7897091722595078,
      "grad_norm": 13.409040451049805,
      "learning_rate": 1.9285011185682327e-05,
      "loss": 1.0618,
      "step": 800
    },
    {
      "epoch": 1.901565995525727,
      "grad_norm": 19.393102645874023,
      "learning_rate": 1.924026845637584e-05,
      "loss": 1.0167,
      "step": 850
    },
    {
      "epoch": 2.0134228187919465,
      "grad_norm": 28.077646255493164,
      "learning_rate": 1.9195525727069353e-05,
      "loss": 0.9624,
      "step": 900
    },
    {
      "epoch": 2.1252796420581657,
      "grad_norm": 30.09161949157715,
      "learning_rate": 1.9150782997762864e-05,
      "loss": 0.8726,
      "step": 950
    },
    {
      "epoch": 2.237136465324385,
      "grad_norm": 24.998777389526367,
      "learning_rate": 1.910604026845638e-05,
      "loss": 0.8503,
      "step": 1000
    },
    {
      "epoch": 2.348993288590604,
      "grad_norm": 19.462141036987305,
      "learning_rate": 1.906129753914989e-05,
      "loss": 0.8256,
      "step": 1050
    },
    {
      "epoch": 2.460850111856823,
      "grad_norm": 22.713390350341797,
      "learning_rate": 1.90165548098434e-05,
      "loss": 0.8191,
      "step": 1100
    },
    {
      "epoch": 2.5727069351230423,
      "grad_norm": 24.602434158325195,
      "learning_rate": 1.8971812080536912e-05,
      "loss": 0.7862,
      "step": 1150
    },
    {
      "epoch": 2.684563758389262,
      "grad_norm": 21.72577667236328,
      "learning_rate": 1.8927069351230427e-05,
      "loss": 0.7942,
      "step": 1200
    },
    {
      "epoch": 2.796420581655481,
      "grad_norm": 13.361296653747559,
      "learning_rate": 1.8882326621923938e-05,
      "loss": 0.7528,
      "step": 1250
    },
    {
      "epoch": 2.9082774049217,
      "grad_norm": 23.873876571655273,
      "learning_rate": 1.883758389261745e-05,
      "loss": 0.7359,
      "step": 1300
    },
    {
      "epoch": 3.0201342281879193,
      "grad_norm": 42.186832427978516,
      "learning_rate": 1.8792841163310964e-05,
      "loss": 0.7295,
      "step": 1350
    },
    {
      "epoch": 3.131991051454139,
      "grad_norm": 22.18952751159668,
      "learning_rate": 1.8748098434004475e-05,
      "loss": 0.6693,
      "step": 1400
    },
    {
      "epoch": 3.243847874720358,
      "grad_norm": 27.97907257080078,
      "learning_rate": 1.8703355704697986e-05,
      "loss": 0.6849,
      "step": 1450
    },
    {
      "epoch": 3.3557046979865772,
      "grad_norm": 67.11878204345703,
      "learning_rate": 1.86586129753915e-05,
      "loss": 0.6561,
      "step": 1500
    },
    {
      "epoch": 3.4675615212527964,
      "grad_norm": 30.1297607421875,
      "learning_rate": 1.8613870246085012e-05,
      "loss": 0.677,
      "step": 1550
    },
    {
      "epoch": 3.5794183445190155,
      "grad_norm": 26.353181838989258,
      "learning_rate": 1.8569127516778523e-05,
      "loss": 0.6562,
      "step": 1600
    },
    {
      "epoch": 3.6912751677852347,
      "grad_norm": 18.21514129638672,
      "learning_rate": 1.8524384787472038e-05,
      "loss": 0.6538,
      "step": 1650
    },
    {
      "epoch": 3.8031319910514543,
      "grad_norm": 23.212772369384766,
      "learning_rate": 1.847964205816555e-05,
      "loss": 0.6382,
      "step": 1700
    },
    {
      "epoch": 3.9149888143176734,
      "grad_norm": 22.81875991821289,
      "learning_rate": 1.843489932885906e-05,
      "loss": 0.6362,
      "step": 1750
    },
    {
      "epoch": 4.026845637583893,
      "grad_norm": 20.71111488342285,
      "learning_rate": 1.839015659955257e-05,
      "loss": 0.6208,
      "step": 1800
    },
    {
      "epoch": 4.138702460850112,
      "grad_norm": 33.19404220581055,
      "learning_rate": 1.8345413870246086e-05,
      "loss": 0.605,
      "step": 1850
    },
    {
      "epoch": 4.250559284116331,
      "grad_norm": 31.623058319091797,
      "learning_rate": 1.8300671140939597e-05,
      "loss": 0.575,
      "step": 1900
    },
    {
      "epoch": 4.3624161073825505,
      "grad_norm": 18.501392364501953,
      "learning_rate": 1.8255928411633112e-05,
      "loss": 0.6307,
      "step": 1950
    },
    {
      "epoch": 4.47427293064877,
      "grad_norm": 38.37204360961914,
      "learning_rate": 1.8211185682326623e-05,
      "loss": 0.5833,
      "step": 2000
    },
    {
      "epoch": 4.586129753914989,
      "grad_norm": 18.808900833129883,
      "learning_rate": 1.8166442953020135e-05,
      "loss": 0.564,
      "step": 2050
    },
    {
      "epoch": 4.697986577181208,
      "grad_norm": 19.31883430480957,
      "learning_rate": 1.812170022371365e-05,
      "loss": 0.5638,
      "step": 2100
    },
    {
      "epoch": 4.809843400447427,
      "grad_norm": 16.568187713623047,
      "learning_rate": 1.807695749440716e-05,
      "loss": 0.5614,
      "step": 2150
    },
    {
      "epoch": 4.921700223713646,
      "grad_norm": 52.67171096801758,
      "learning_rate": 1.803221476510067e-05,
      "loss": 0.5582,
      "step": 2200
    },
    {
      "epoch": 5.033557046979865,
      "grad_norm": 23.394779205322266,
      "learning_rate": 1.7987472035794186e-05,
      "loss": 0.5412,
      "step": 2250
    },
    {
      "epoch": 5.145413870246085,
      "grad_norm": 23.900815963745117,
      "learning_rate": 1.7942729306487697e-05,
      "loss": 0.5418,
      "step": 2300
    },
    {
      "epoch": 5.257270693512305,
      "grad_norm": 20.412981033325195,
      "learning_rate": 1.789798657718121e-05,
      "loss": 0.5208,
      "step": 2350
    },
    {
      "epoch": 5.369127516778524,
      "grad_norm": 13.53274917602539,
      "learning_rate": 1.7853243847874723e-05,
      "loss": 0.5083,
      "step": 2400
    },
    {
      "epoch": 5.480984340044743,
      "grad_norm": 23.42705726623535,
      "learning_rate": 1.7808501118568234e-05,
      "loss": 0.5473,
      "step": 2450
    },
    {
      "epoch": 5.592841163310962,
      "grad_norm": 22.551162719726562,
      "learning_rate": 1.7763758389261746e-05,
      "loss": 0.5196,
      "step": 2500
    },
    {
      "epoch": 5.704697986577181,
      "grad_norm": 41.93785858154297,
      "learning_rate": 1.771901565995526e-05,
      "loss": 0.4955,
      "step": 2550
    },
    {
      "epoch": 5.8165548098434,
      "grad_norm": 72.0236587524414,
      "learning_rate": 1.767427293064877e-05,
      "loss": 0.5022,
      "step": 2600
    },
    {
      "epoch": 5.9284116331096195,
      "grad_norm": 25.99888801574707,
      "learning_rate": 1.7629530201342283e-05,
      "loss": 0.4984,
      "step": 2650
    },
    {
      "epoch": 6.040268456375839,
      "grad_norm": 25.423633575439453,
      "learning_rate": 1.7584787472035797e-05,
      "loss": 0.4634,
      "step": 2700
    },
    {
      "epoch": 6.152125279642058,
      "grad_norm": 21.017425537109375,
      "learning_rate": 1.754004474272931e-05,
      "loss": 0.4699,
      "step": 2750
    },
    {
      "epoch": 6.263982102908278,
      "grad_norm": 20.92989158630371,
      "learning_rate": 1.749530201342282e-05,
      "loss": 0.4849,
      "step": 2800
    },
    {
      "epoch": 6.375838926174497,
      "grad_norm": 15.407737731933594,
      "learning_rate": 1.7450559284116334e-05,
      "loss": 0.4825,
      "step": 2850
    },
    {
      "epoch": 6.487695749440716,
      "grad_norm": 19.01060676574707,
      "learning_rate": 1.7405816554809846e-05,
      "loss": 0.4738,
      "step": 2900
    },
    {
      "epoch": 6.599552572706935,
      "grad_norm": 17.438276290893555,
      "learning_rate": 1.7361073825503357e-05,
      "loss": 0.4847,
      "step": 2950
    },
    {
      "epoch": 6.7114093959731544,
      "grad_norm": 18.991146087646484,
      "learning_rate": 1.731633109619687e-05,
      "loss": 0.4689,
      "step": 3000
    },
    {
      "epoch": 6.823266219239374,
      "grad_norm": 25.318912506103516,
      "learning_rate": 1.7271588366890383e-05,
      "loss": 0.4675,
      "step": 3050
    },
    {
      "epoch": 6.935123042505593,
      "grad_norm": 43.093666076660156,
      "learning_rate": 1.7226845637583894e-05,
      "loss": 0.4654,
      "step": 3100
    },
    {
      "epoch": 7.046979865771812,
      "grad_norm": 26.078580856323242,
      "learning_rate": 1.718210290827741e-05,
      "loss": 0.4692,
      "step": 3150
    },
    {
      "epoch": 7.158836689038031,
      "grad_norm": 24.1142635345459,
      "learning_rate": 1.713736017897092e-05,
      "loss": 0.4583,
      "step": 3200
    },
    {
      "epoch": 7.27069351230425,
      "grad_norm": 38.3083381652832,
      "learning_rate": 1.709261744966443e-05,
      "loss": 0.4308,
      "step": 3250
    },
    {
      "epoch": 7.382550335570469,
      "grad_norm": 23.089950561523438,
      "learning_rate": 1.7047874720357946e-05,
      "loss": 0.4503,
      "step": 3300
    },
    {
      "epoch": 7.494407158836689,
      "grad_norm": 19.32261848449707,
      "learning_rate": 1.7003131991051457e-05,
      "loss": 0.4449,
      "step": 3350
    },
    {
      "epoch": 7.6062639821029085,
      "grad_norm": 13.630754470825195,
      "learning_rate": 1.6958389261744968e-05,
      "loss": 0.4245,
      "step": 3400
    },
    {
      "epoch": 7.718120805369128,
      "grad_norm": 20.614797592163086,
      "learning_rate": 1.691364653243848e-05,
      "loss": 0.4165,
      "step": 3450
    },
    {
      "epoch": 7.829977628635347,
      "grad_norm": 23.484899520874023,
      "learning_rate": 1.6868903803131994e-05,
      "loss": 0.402,
      "step": 3500
    },
    {
      "epoch": 7.941834451901566,
      "grad_norm": 30.247102737426758,
      "learning_rate": 1.6824161073825505e-05,
      "loss": 0.4414,
      "step": 3550
    },
    {
      "epoch": 8.053691275167786,
      "grad_norm": 25.080188751220703,
      "learning_rate": 1.6779418344519016e-05,
      "loss": 0.4065,
      "step": 3600
    },
    {
      "epoch": 8.165548098434005,
      "grad_norm": 32.94839859008789,
      "learning_rate": 1.673467561521253e-05,
      "loss": 0.4211,
      "step": 3650
    },
    {
      "epoch": 8.277404921700224,
      "grad_norm": 23.89237403869629,
      "learning_rate": 1.6689932885906042e-05,
      "loss": 0.4073,
      "step": 3700
    },
    {
      "epoch": 8.389261744966444,
      "grad_norm": 13.891432762145996,
      "learning_rate": 1.6645190156599553e-05,
      "loss": 0.3991,
      "step": 3750
    },
    {
      "epoch": 8.501118568232663,
      "grad_norm": 15.971633911132812,
      "learning_rate": 1.6600447427293068e-05,
      "loss": 0.4074,
      "step": 3800
    },
    {
      "epoch": 8.612975391498882,
      "grad_norm": 27.01921272277832,
      "learning_rate": 1.655570469798658e-05,
      "loss": 0.3893,
      "step": 3850
    },
    {
      "epoch": 8.724832214765101,
      "grad_norm": 19.01692771911621,
      "learning_rate": 1.651096196868009e-05,
      "loss": 0.4087,
      "step": 3900
    },
    {
      "epoch": 8.83668903803132,
      "grad_norm": 16.597673416137695,
      "learning_rate": 1.6466219239373605e-05,
      "loss": 0.4033,
      "step": 3950
    },
    {
      "epoch": 8.94854586129754,
      "grad_norm": 15.752256393432617,
      "learning_rate": 1.6421476510067116e-05,
      "loss": 0.3746,
      "step": 4000
    },
    {
      "epoch": 9.060402684563758,
      "grad_norm": 13.994440078735352,
      "learning_rate": 1.6376733780760628e-05,
      "loss": 0.3697,
      "step": 4050
    },
    {
      "epoch": 9.172259507829978,
      "grad_norm": 19.512922286987305,
      "learning_rate": 1.633199105145414e-05,
      "loss": 0.3762,
      "step": 4100
    },
    {
      "epoch": 9.284116331096197,
      "grad_norm": 74.67884063720703,
      "learning_rate": 1.6287248322147653e-05,
      "loss": 0.3882,
      "step": 4150
    },
    {
      "epoch": 9.395973154362416,
      "grad_norm": 11.065535545349121,
      "learning_rate": 1.6242505592841165e-05,
      "loss": 0.3543,
      "step": 4200
    },
    {
      "epoch": 9.507829977628635,
      "grad_norm": 18.210031509399414,
      "learning_rate": 1.6197762863534676e-05,
      "loss": 0.3843,
      "step": 4250
    },
    {
      "epoch": 9.619686800894854,
      "grad_norm": 21.382043838500977,
      "learning_rate": 1.615302013422819e-05,
      "loss": 0.3901,
      "step": 4300
    },
    {
      "epoch": 9.731543624161073,
      "grad_norm": 23.012210845947266,
      "learning_rate": 1.61082774049217e-05,
      "loss": 0.3799,
      "step": 4350
    },
    {
      "epoch": 9.843400447427292,
      "grad_norm": 27.78239631652832,
      "learning_rate": 1.6063534675615213e-05,
      "loss": 0.348,
      "step": 4400
    },
    {
      "epoch": 9.955257270693512,
      "grad_norm": 21.54998207092285,
      "learning_rate": 1.6018791946308727e-05,
      "loss": 0.369,
      "step": 4450
    },
    {
      "epoch": 10.06711409395973,
      "grad_norm": 13.635636329650879,
      "learning_rate": 1.597404921700224e-05,
      "loss": 0.3578,
      "step": 4500
    },
    {
      "epoch": 10.17897091722595,
      "grad_norm": 20.80027961730957,
      "learning_rate": 1.592930648769575e-05,
      "loss": 0.3408,
      "step": 4550
    },
    {
      "epoch": 10.29082774049217,
      "grad_norm": 12.54713249206543,
      "learning_rate": 1.588456375838926e-05,
      "loss": 0.342,
      "step": 4600
    },
    {
      "epoch": 10.40268456375839,
      "grad_norm": 27.841732025146484,
      "learning_rate": 1.5839821029082776e-05,
      "loss": 0.3495,
      "step": 4650
    },
    {
      "epoch": 10.51454138702461,
      "grad_norm": 24.27332878112793,
      "learning_rate": 1.5795078299776287e-05,
      "loss": 0.356,
      "step": 4700
    },
    {
      "epoch": 10.626398210290828,
      "grad_norm": 25.755786895751953,
      "learning_rate": 1.5750335570469798e-05,
      "loss": 0.3371,
      "step": 4750
    },
    {
      "epoch": 10.738255033557047,
      "grad_norm": 29.45204734802246,
      "learning_rate": 1.5705592841163313e-05,
      "loss": 0.3716,
      "step": 4800
    },
    {
      "epoch": 10.850111856823267,
      "grad_norm": 11.651004791259766,
      "learning_rate": 1.5660850111856824e-05,
      "loss": 0.3434,
      "step": 4850
    },
    {
      "epoch": 10.961968680089486,
      "grad_norm": 21.144865036010742,
      "learning_rate": 1.5616107382550335e-05,
      "loss": 0.3569,
      "step": 4900
    },
    {
      "epoch": 11.073825503355705,
      "grad_norm": 25.662721633911133,
      "learning_rate": 1.557136465324385e-05,
      "loss": 0.3293,
      "step": 4950
    },
    {
      "epoch": 11.185682326621924,
      "grad_norm": 25.06239891052246,
      "learning_rate": 1.552662192393736e-05,
      "loss": 0.3264,
      "step": 5000
    },
    {
      "epoch": 11.297539149888143,
      "grad_norm": 29.008502960205078,
      "learning_rate": 1.5481879194630872e-05,
      "loss": 0.3167,
      "step": 5050
    },
    {
      "epoch": 11.409395973154362,
      "grad_norm": 25.332815170288086,
      "learning_rate": 1.5437136465324387e-05,
      "loss": 0.308,
      "step": 5100
    },
    {
      "epoch": 11.521252796420582,
      "grad_norm": 19.524877548217773,
      "learning_rate": 1.5392393736017898e-05,
      "loss": 0.3289,
      "step": 5150
    },
    {
      "epoch": 11.6331096196868,
      "grad_norm": 68.673095703125,
      "learning_rate": 1.534765100671141e-05,
      "loss": 0.3529,
      "step": 5200
    },
    {
      "epoch": 11.74496644295302,
      "grad_norm": 16.947641372680664,
      "learning_rate": 1.530290827740492e-05,
      "loss": 0.347,
      "step": 5250
    },
    {
      "epoch": 11.856823266219239,
      "grad_norm": 21.9144344329834,
      "learning_rate": 1.5258165548098435e-05,
      "loss": 0.3363,
      "step": 5300
    },
    {
      "epoch": 11.968680089485458,
      "grad_norm": 17.15624237060547,
      "learning_rate": 1.5213422818791948e-05,
      "loss": 0.328,
      "step": 5350
    },
    {
      "epoch": 12.080536912751677,
      "grad_norm": 106.93811798095703,
      "learning_rate": 1.516868008948546e-05,
      "loss": 0.3237,
      "step": 5400
    },
    {
      "epoch": 12.192393736017896,
      "grad_norm": 13.129621505737305,
      "learning_rate": 1.5123937360178972e-05,
      "loss": 0.3123,
      "step": 5450
    },
    {
      "epoch": 12.304250559284116,
      "grad_norm": 25.352886199951172,
      "learning_rate": 1.5079194630872485e-05,
      "loss": 0.3133,
      "step": 5500
    },
    {
      "epoch": 12.416107382550335,
      "grad_norm": 17.82615852355957,
      "learning_rate": 1.5034451901565996e-05,
      "loss": 0.3269,
      "step": 5550
    },
    {
      "epoch": 12.527964205816556,
      "grad_norm": 26.06785011291504,
      "learning_rate": 1.498970917225951e-05,
      "loss": 0.33,
      "step": 5600
    },
    {
      "epoch": 12.639821029082775,
      "grad_norm": 25.77390480041504,
      "learning_rate": 1.4944966442953022e-05,
      "loss": 0.297,
      "step": 5650
    },
    {
      "epoch": 12.751677852348994,
      "grad_norm": 17.756752014160156,
      "learning_rate": 1.4900223713646533e-05,
      "loss": 0.3034,
      "step": 5700
    },
    {
      "epoch": 12.863534675615213,
      "grad_norm": 14.373438835144043,
      "learning_rate": 1.4855480984340045e-05,
      "loss": 0.2941,
      "step": 5750
    },
    {
      "epoch": 12.975391498881432,
      "grad_norm": 27.280057907104492,
      "learning_rate": 1.481073825503356e-05,
      "loss": 0.2826,
      "step": 5800
    },
    {
      "epoch": 13.087248322147651,
      "grad_norm": 23.134765625,
      "learning_rate": 1.476599552572707e-05,
      "loss": 0.3231,
      "step": 5850
    },
    {
      "epoch": 13.19910514541387,
      "grad_norm": 22.549577713012695,
      "learning_rate": 1.4721252796420582e-05,
      "loss": 0.3004,
      "step": 5900
    },
    {
      "epoch": 13.31096196868009,
      "grad_norm": 40.20413589477539,
      "learning_rate": 1.4676510067114096e-05,
      "loss": 0.311,
      "step": 5950
    },
    {
      "epoch": 13.422818791946309,
      "grad_norm": 21.152629852294922,
      "learning_rate": 1.4631767337807608e-05,
      "loss": 0.3045,
      "step": 6000
    },
    {
      "epoch": 13.534675615212528,
      "grad_norm": 16.182621002197266,
      "learning_rate": 1.4587024608501119e-05,
      "loss": 0.2753,
      "step": 6050
    },
    {
      "epoch": 13.646532438478747,
      "grad_norm": 17.503925323486328,
      "learning_rate": 1.4542281879194633e-05,
      "loss": 0.2813,
      "step": 6100
    },
    {
      "epoch": 13.758389261744966,
      "grad_norm": 26.20952033996582,
      "learning_rate": 1.4497539149888145e-05,
      "loss": 0.3012,
      "step": 6150
    },
    {
      "epoch": 13.870246085011185,
      "grad_norm": 22.830907821655273,
      "learning_rate": 1.4452796420581656e-05,
      "loss": 0.2944,
      "step": 6200
    },
    {
      "epoch": 13.982102908277405,
      "grad_norm": 36.29288864135742,
      "learning_rate": 1.440805369127517e-05,
      "loss": 0.2742,
      "step": 6250
    },
    {
      "epoch": 14.093959731543624,
      "grad_norm": 13.733006477355957,
      "learning_rate": 1.4363310961968682e-05,
      "loss": 0.3022,
      "step": 6300
    },
    {
      "epoch": 14.205816554809843,
      "grad_norm": 18.309154510498047,
      "learning_rate": 1.4318568232662193e-05,
      "loss": 0.2917,
      "step": 6350
    },
    {
      "epoch": 14.317673378076062,
      "grad_norm": 14.842955589294434,
      "learning_rate": 1.4273825503355704e-05,
      "loss": 0.2641,
      "step": 6400
    },
    {
      "epoch": 14.429530201342281,
      "grad_norm": 28.093868255615234,
      "learning_rate": 1.4229082774049219e-05,
      "loss": 0.2805,
      "step": 6450
    },
    {
      "epoch": 14.5413870246085,
      "grad_norm": 13.566606521606445,
      "learning_rate": 1.418434004474273e-05,
      "loss": 0.2861,
      "step": 6500
    },
    {
      "epoch": 14.65324384787472,
      "grad_norm": 21.004383087158203,
      "learning_rate": 1.4139597315436241e-05,
      "loss": 0.2697,
      "step": 6550
    },
    {
      "epoch": 14.765100671140939,
      "grad_norm": 20.80152130126953,
      "learning_rate": 1.4094854586129756e-05,
      "loss": 0.302,
      "step": 6600
    },
    {
      "epoch": 14.87695749440716,
      "grad_norm": 18.016464233398438,
      "learning_rate": 1.4050111856823267e-05,
      "loss": 0.283,
      "step": 6650
    },
    {
      "epoch": 14.988814317673379,
      "grad_norm": 13.152626991271973,
      "learning_rate": 1.4005369127516778e-05,
      "loss": 0.2657,
      "step": 6700
    },
    {
      "epoch": 15.100671140939598,
      "grad_norm": 18.9659481048584,
      "learning_rate": 1.3960626398210293e-05,
      "loss": 0.2563,
      "step": 6750
    },
    {
      "epoch": 15.212527964205817,
      "grad_norm": 27.123777389526367,
      "learning_rate": 1.3915883668903804e-05,
      "loss": 0.2745,
      "step": 6800
    },
    {
      "epoch": 15.324384787472036,
      "grad_norm": 23.286880493164062,
      "learning_rate": 1.3871140939597317e-05,
      "loss": 0.2555,
      "step": 6850
    },
    {
      "epoch": 15.436241610738255,
      "grad_norm": 17.858903884887695,
      "learning_rate": 1.3826398210290828e-05,
      "loss": 0.2714,
      "step": 6900
    },
    {
      "epoch": 15.548098434004475,
      "grad_norm": 24.958660125732422,
      "learning_rate": 1.3781655480984341e-05,
      "loss": 0.2453,
      "step": 6950
    },
    {
      "epoch": 15.659955257270694,
      "grad_norm": 12.101340293884277,
      "learning_rate": 1.3736912751677854e-05,
      "loss": 0.257,
      "step": 7000
    },
    {
      "epoch": 15.771812080536913,
      "grad_norm": 14.337169647216797,
      "learning_rate": 1.3692170022371365e-05,
      "loss": 0.2643,
      "step": 7050
    },
    {
      "epoch": 15.883668903803132,
      "grad_norm": 19.752235412597656,
      "learning_rate": 1.3647427293064878e-05,
      "loss": 0.275,
      "step": 7100
    },
    {
      "epoch": 15.995525727069351,
      "grad_norm": 28.628995895385742,
      "learning_rate": 1.3602684563758391e-05,
      "loss": 0.2706,
      "step": 7150
    },
    {
      "epoch": 16.107382550335572,
      "grad_norm": 20.540708541870117,
      "learning_rate": 1.3557941834451902e-05,
      "loss": 0.2486,
      "step": 7200
    },
    {
      "epoch": 16.21923937360179,
      "grad_norm": 22.900266647338867,
      "learning_rate": 1.3513199105145415e-05,
      "loss": 0.2506,
      "step": 7250
    },
    {
      "epoch": 16.33109619686801,
      "grad_norm": 15.41297435760498,
      "learning_rate": 1.3468456375838928e-05,
      "loss": 0.2576,
      "step": 7300
    },
    {
      "epoch": 16.44295302013423,
      "grad_norm": 20.893932342529297,
      "learning_rate": 1.342371364653244e-05,
      "loss": 0.2505,
      "step": 7350
    },
    {
      "epoch": 16.55480984340045,
      "grad_norm": 30.079458236694336,
      "learning_rate": 1.3378970917225952e-05,
      "loss": 0.2352,
      "step": 7400
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 12.368570327758789,
      "learning_rate": 1.3334228187919465e-05,
      "loss": 0.2497,
      "step": 7450
    },
    {
      "epoch": 16.778523489932887,
      "grad_norm": 64.36371612548828,
      "learning_rate": 1.3289485458612977e-05,
      "loss": 0.2487,
      "step": 7500
    },
    {
      "epoch": 16.890380313199106,
      "grad_norm": 20.929231643676758,
      "learning_rate": 1.3244742729306488e-05,
      "loss": 0.2465,
      "step": 7550
    },
    {
      "epoch": 17.002237136465325,
      "grad_norm": 19.772300720214844,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 0.2575,
      "step": 7600
    },
    {
      "epoch": 17.114093959731544,
      "grad_norm": 25.304187774658203,
      "learning_rate": 1.3155257270693514e-05,
      "loss": 0.2402,
      "step": 7650
    },
    {
      "epoch": 17.225950782997764,
      "grad_norm": 18.17524528503418,
      "learning_rate": 1.3110514541387025e-05,
      "loss": 0.2422,
      "step": 7700
    },
    {
      "epoch": 17.337807606263983,
      "grad_norm": 104.07044982910156,
      "learning_rate": 1.306577181208054e-05,
      "loss": 0.2426,
      "step": 7750
    },
    {
      "epoch": 17.449664429530202,
      "grad_norm": 16.416746139526367,
      "learning_rate": 1.302102908277405e-05,
      "loss": 0.2481,
      "step": 7800
    },
    {
      "epoch": 17.56152125279642,
      "grad_norm": 16.397972106933594,
      "learning_rate": 1.2976286353467562e-05,
      "loss": 0.2261,
      "step": 7850
    },
    {
      "epoch": 17.67337807606264,
      "grad_norm": 19.322677612304688,
      "learning_rate": 1.2931543624161076e-05,
      "loss": 0.2464,
      "step": 7900
    },
    {
      "epoch": 17.78523489932886,
      "grad_norm": 36.2310676574707,
      "learning_rate": 1.2886800894854588e-05,
      "loss": 0.2475,
      "step": 7950
    },
    {
      "epoch": 17.89709172259508,
      "grad_norm": 23.82117462158203,
      "learning_rate": 1.2842058165548099e-05,
      "loss": 0.2619,
      "step": 8000
    },
    {
      "epoch": 18.008948545861298,
      "grad_norm": 16.39063835144043,
      "learning_rate": 1.279731543624161e-05,
      "loss": 0.2322,
      "step": 8050
    },
    {
      "epoch": 18.120805369127517,
      "grad_norm": 136.73117065429688,
      "learning_rate": 1.2752572706935125e-05,
      "loss": 0.2206,
      "step": 8100
    },
    {
      "epoch": 18.232662192393736,
      "grad_norm": 32.329559326171875,
      "learning_rate": 1.2707829977628636e-05,
      "loss": 0.2271,
      "step": 8150
    },
    {
      "epoch": 18.344519015659955,
      "grad_norm": 19.833080291748047,
      "learning_rate": 1.2663087248322147e-05,
      "loss": 0.2237,
      "step": 8200
    },
    {
      "epoch": 18.456375838926174,
      "grad_norm": 20.227294921875,
      "learning_rate": 1.2618344519015662e-05,
      "loss": 0.2232,
      "step": 8250
    },
    {
      "epoch": 18.568232662192393,
      "grad_norm": 22.033815383911133,
      "learning_rate": 1.2573601789709173e-05,
      "loss": 0.2228,
      "step": 8300
    },
    {
      "epoch": 18.680089485458613,
      "grad_norm": 22.627534866333008,
      "learning_rate": 1.2528859060402684e-05,
      "loss": 0.2436,
      "step": 8350
    },
    {
      "epoch": 18.79194630872483,
      "grad_norm": 9.75200366973877,
      "learning_rate": 1.2484116331096199e-05,
      "loss": 0.2282,
      "step": 8400
    },
    {
      "epoch": 18.90380313199105,
      "grad_norm": 10.110821723937988,
      "learning_rate": 1.243937360178971e-05,
      "loss": 0.2195,
      "step": 8450
    },
    {
      "epoch": 19.01565995525727,
      "grad_norm": 27.38565444946289,
      "learning_rate": 1.2394630872483221e-05,
      "loss": 0.2572,
      "step": 8500
    },
    {
      "epoch": 19.12751677852349,
      "grad_norm": 17.372777938842773,
      "learning_rate": 1.2349888143176736e-05,
      "loss": 0.2047,
      "step": 8550
    },
    {
      "epoch": 19.23937360178971,
      "grad_norm": 19.96657943725586,
      "learning_rate": 1.2305145413870247e-05,
      "loss": 0.2243,
      "step": 8600
    },
    {
      "epoch": 19.351230425055927,
      "grad_norm": 23.265838623046875,
      "learning_rate": 1.2260402684563758e-05,
      "loss": 0.2232,
      "step": 8650
    },
    {
      "epoch": 19.463087248322147,
      "grad_norm": 23.529531478881836,
      "learning_rate": 1.2215659955257271e-05,
      "loss": 0.2083,
      "step": 8700
    },
    {
      "epoch": 19.574944071588366,
      "grad_norm": 11.927926063537598,
      "learning_rate": 1.2170917225950784e-05,
      "loss": 0.2299,
      "step": 8750
    },
    {
      "epoch": 19.686800894854585,
      "grad_norm": 11.954673767089844,
      "learning_rate": 1.2126174496644295e-05,
      "loss": 0.2094,
      "step": 8800
    },
    {
      "epoch": 19.798657718120804,
      "grad_norm": 14.966828346252441,
      "learning_rate": 1.2081431767337808e-05,
      "loss": 0.2127,
      "step": 8850
    },
    {
      "epoch": 19.910514541387023,
      "grad_norm": 35.19870376586914,
      "learning_rate": 1.2036689038031321e-05,
      "loss": 0.2284,
      "step": 8900
    },
    {
      "epoch": 20.022371364653242,
      "grad_norm": 22.85268783569336,
      "learning_rate": 1.1991946308724833e-05,
      "loss": 0.2197,
      "step": 8950
    },
    {
      "epoch": 20.13422818791946,
      "grad_norm": 24.724714279174805,
      "learning_rate": 1.1947203579418345e-05,
      "loss": 0.2289,
      "step": 9000
    },
    {
      "epoch": 20.24608501118568,
      "grad_norm": 23.6364688873291,
      "learning_rate": 1.1902460850111858e-05,
      "loss": 0.2224,
      "step": 9050
    },
    {
      "epoch": 20.3579418344519,
      "grad_norm": 20.506332397460938,
      "learning_rate": 1.1857718120805371e-05,
      "loss": 0.1949,
      "step": 9100
    },
    {
      "epoch": 20.469798657718123,
      "grad_norm": 17.683406829833984,
      "learning_rate": 1.1812975391498883e-05,
      "loss": 0.2122,
      "step": 9150
    },
    {
      "epoch": 20.58165548098434,
      "grad_norm": 23.797178268432617,
      "learning_rate": 1.1768232662192394e-05,
      "loss": 0.2234,
      "step": 9200
    },
    {
      "epoch": 20.69351230425056,
      "grad_norm": 12.691411018371582,
      "learning_rate": 1.1723489932885908e-05,
      "loss": 0.2147,
      "step": 9250
    },
    {
      "epoch": 20.80536912751678,
      "grad_norm": 184.8130645751953,
      "learning_rate": 1.167874720357942e-05,
      "loss": 0.1981,
      "step": 9300
    },
    {
      "epoch": 20.917225950783,
      "grad_norm": 12.475272178649902,
      "learning_rate": 1.163400447427293e-05,
      "loss": 0.2087,
      "step": 9350
    },
    {
      "epoch": 21.02908277404922,
      "grad_norm": 142.87197875976562,
      "learning_rate": 1.1589261744966445e-05,
      "loss": 0.2006,
      "step": 9400
    },
    {
      "epoch": 21.140939597315437,
      "grad_norm": 47.383644104003906,
      "learning_rate": 1.1544519015659957e-05,
      "loss": 0.2088,
      "step": 9450
    },
    {
      "epoch": 21.252796420581657,
      "grad_norm": 18.24808692932129,
      "learning_rate": 1.1499776286353468e-05,
      "loss": 0.211,
      "step": 9500
    },
    {
      "epoch": 21.364653243847876,
      "grad_norm": 14.36365795135498,
      "learning_rate": 1.1455033557046982e-05,
      "loss": 0.206,
      "step": 9550
    },
    {
      "epoch": 21.476510067114095,
      "grad_norm": 14.142335891723633,
      "learning_rate": 1.1410290827740494e-05,
      "loss": 0.2012,
      "step": 9600
    },
    {
      "epoch": 21.588366890380314,
      "grad_norm": 17.758085250854492,
      "learning_rate": 1.1365548098434005e-05,
      "loss": 0.2148,
      "step": 9650
    },
    {
      "epoch": 21.700223713646533,
      "grad_norm": 16.298295974731445,
      "learning_rate": 1.1320805369127516e-05,
      "loss": 0.2042,
      "step": 9700
    },
    {
      "epoch": 21.812080536912752,
      "grad_norm": 29.684226989746094,
      "learning_rate": 1.127606263982103e-05,
      "loss": 0.1964,
      "step": 9750
    },
    {
      "epoch": 21.92393736017897,
      "grad_norm": 14.867528915405273,
      "learning_rate": 1.1231319910514542e-05,
      "loss": 0.1952,
      "step": 9800
    },
    {
      "epoch": 22.03579418344519,
      "grad_norm": 21.78657341003418,
      "learning_rate": 1.1186577181208053e-05,
      "loss": 0.1863,
      "step": 9850
    },
    {
      "epoch": 22.14765100671141,
      "grad_norm": 19.78911018371582,
      "learning_rate": 1.1141834451901568e-05,
      "loss": 0.1956,
      "step": 9900
    },
    {
      "epoch": 22.25950782997763,
      "grad_norm": 12.803306579589844,
      "learning_rate": 1.1097091722595079e-05,
      "loss": 0.1912,
      "step": 9950
    },
    {
      "epoch": 22.371364653243848,
      "grad_norm": 14.830564498901367,
      "learning_rate": 1.105234899328859e-05,
      "loss": 0.2023,
      "step": 10000
    },
    {
      "epoch": 22.483221476510067,
      "grad_norm": 17.404661178588867,
      "learning_rate": 1.1007606263982105e-05,
      "loss": 0.1808,
      "step": 10050
    },
    {
      "epoch": 22.595078299776286,
      "grad_norm": 22.68096923828125,
      "learning_rate": 1.0962863534675616e-05,
      "loss": 0.1898,
      "step": 10100
    },
    {
      "epoch": 22.706935123042506,
      "grad_norm": 22.085906982421875,
      "learning_rate": 1.0918120805369127e-05,
      "loss": 0.1758,
      "step": 10150
    },
    {
      "epoch": 22.818791946308725,
      "grad_norm": 27.175792694091797,
      "learning_rate": 1.0873378076062642e-05,
      "loss": 0.2034,
      "step": 10200
    },
    {
      "epoch": 22.930648769574944,
      "grad_norm": 50.80461883544922,
      "learning_rate": 1.0828635346756153e-05,
      "loss": 0.2031,
      "step": 10250
    },
    {
      "epoch": 23.042505592841163,
      "grad_norm": 43.474769592285156,
      "learning_rate": 1.0783892617449664e-05,
      "loss": 0.1857,
      "step": 10300
    },
    {
      "epoch": 23.154362416107382,
      "grad_norm": 30.666221618652344,
      "learning_rate": 1.0739149888143177e-05,
      "loss": 0.1906,
      "step": 10350
    },
    {
      "epoch": 23.2662192393736,
      "grad_norm": 139.18478393554688,
      "learning_rate": 1.069440715883669e-05,
      "loss": 0.1954,
      "step": 10400
    },
    {
      "epoch": 23.37807606263982,
      "grad_norm": 246.9184112548828,
      "learning_rate": 1.0649664429530201e-05,
      "loss": 0.1878,
      "step": 10450
    },
    {
      "epoch": 23.48993288590604,
      "grad_norm": 13.650147438049316,
      "learning_rate": 1.0604921700223714e-05,
      "loss": 0.1935,
      "step": 10500
    },
    {
      "epoch": 23.60178970917226,
      "grad_norm": 63.55646896362305,
      "learning_rate": 1.0560178970917227e-05,
      "loss": 0.1978,
      "step": 10550
    },
    {
      "epoch": 23.713646532438478,
      "grad_norm": 35.29027557373047,
      "learning_rate": 1.0515436241610739e-05,
      "loss": 0.1766,
      "step": 10600
    },
    {
      "epoch": 23.825503355704697,
      "grad_norm": 13.38139820098877,
      "learning_rate": 1.0470693512304251e-05,
      "loss": 0.192,
      "step": 10650
    },
    {
      "epoch": 23.937360178970916,
      "grad_norm": 22.308021545410156,
      "learning_rate": 1.0425950782997764e-05,
      "loss": 0.1883,
      "step": 10700
    },
    {
      "epoch": 24.049217002237135,
      "grad_norm": 21.79499626159668,
      "learning_rate": 1.0381208053691276e-05,
      "loss": 0.184,
      "step": 10750
    },
    {
      "epoch": 24.161073825503355,
      "grad_norm": 23.037446975708008,
      "learning_rate": 1.0336465324384788e-05,
      "loss": 0.1779,
      "step": 10800
    },
    {
      "epoch": 24.272930648769574,
      "grad_norm": 15.866609573364258,
      "learning_rate": 1.02917225950783e-05,
      "loss": 0.1867,
      "step": 10850
    },
    {
      "epoch": 24.384787472035793,
      "grad_norm": 105.06596374511719,
      "learning_rate": 1.0246979865771813e-05,
      "loss": 0.1819,
      "step": 10900
    },
    {
      "epoch": 24.496644295302012,
      "grad_norm": 11.426310539245605,
      "learning_rate": 1.0202237136465326e-05,
      "loss": 0.1842,
      "step": 10950
    },
    {
      "epoch": 24.60850111856823,
      "grad_norm": 14.60068416595459,
      "learning_rate": 1.0157494407158837e-05,
      "loss": 0.1759,
      "step": 11000
    },
    {
      "epoch": 24.72035794183445,
      "grad_norm": 13.649640083312988,
      "learning_rate": 1.011275167785235e-05,
      "loss": 0.1773,
      "step": 11050
    },
    {
      "epoch": 24.83221476510067,
      "grad_norm": 15.199976921081543,
      "learning_rate": 1.0068008948545863e-05,
      "loss": 0.1634,
      "step": 11100
    },
    {
      "epoch": 24.94407158836689,
      "grad_norm": 10.192238807678223,
      "learning_rate": 1.0023266219239374e-05,
      "loss": 0.1914,
      "step": 11150
    },
    {
      "epoch": 25.05592841163311,
      "grad_norm": 9.363759994506836,
      "learning_rate": 9.978523489932887e-06,
      "loss": 0.1724,
      "step": 11200
    },
    {
      "epoch": 25.16778523489933,
      "grad_norm": 35.594818115234375,
      "learning_rate": 9.9337807606264e-06,
      "loss": 0.1674,
      "step": 11250
    },
    {
      "epoch": 25.27964205816555,
      "grad_norm": 23.18999481201172,
      "learning_rate": 9.889038031319911e-06,
      "loss": 0.1864,
      "step": 11300
    },
    {
      "epoch": 25.39149888143177,
      "grad_norm": 18.068824768066406,
      "learning_rate": 9.844295302013424e-06,
      "loss": 0.1785,
      "step": 11350
    },
    {
      "epoch": 25.503355704697988,
      "grad_norm": 170.672607421875,
      "learning_rate": 9.799552572706937e-06,
      "loss": 0.1762,
      "step": 11400
    },
    {
      "epoch": 25.615212527964207,
      "grad_norm": 17.813264846801758,
      "learning_rate": 9.754809843400448e-06,
      "loss": 0.1748,
      "step": 11450
    },
    {
      "epoch": 25.727069351230426,
      "grad_norm": 36.61251449584961,
      "learning_rate": 9.710067114093961e-06,
      "loss": 0.1845,
      "step": 11500
    },
    {
      "epoch": 25.838926174496645,
      "grad_norm": 22.625022888183594,
      "learning_rate": 9.665324384787474e-06,
      "loss": 0.1849,
      "step": 11550
    },
    {
      "epoch": 25.950782997762865,
      "grad_norm": 18.912900924682617,
      "learning_rate": 9.620581655480985e-06,
      "loss": 0.1809,
      "step": 11600
    },
    {
      "epoch": 26.062639821029084,
      "grad_norm": 30.787967681884766,
      "learning_rate": 9.575838926174498e-06,
      "loss": 0.1753,
      "step": 11650
    },
    {
      "epoch": 26.174496644295303,
      "grad_norm": 26.441083908081055,
      "learning_rate": 9.53109619686801e-06,
      "loss": 0.1642,
      "step": 11700
    },
    {
      "epoch": 26.286353467561522,
      "grad_norm": 9.030888557434082,
      "learning_rate": 9.486353467561522e-06,
      "loss": 0.168,
      "step": 11750
    },
    {
      "epoch": 26.39821029082774,
      "grad_norm": 20.001747131347656,
      "learning_rate": 9.441610738255035e-06,
      "loss": 0.172,
      "step": 11800
    },
    {
      "epoch": 26.51006711409396,
      "grad_norm": 41.12727737426758,
      "learning_rate": 9.396868008948546e-06,
      "loss": 0.1788,
      "step": 11850
    },
    {
      "epoch": 26.62192393736018,
      "grad_norm": 404.3497009277344,
      "learning_rate": 9.352125279642059e-06,
      "loss": 0.1697,
      "step": 11900
    },
    {
      "epoch": 26.7337807606264,
      "grad_norm": 22.781251907348633,
      "learning_rate": 9.30738255033557e-06,
      "loss": 0.1693,
      "step": 11950
    },
    {
      "epoch": 26.845637583892618,
      "grad_norm": 22.5933837890625,
      "learning_rate": 9.262639821029083e-06,
      "loss": 0.1672,
      "step": 12000
    },
    {
      "epoch": 26.957494407158837,
      "grad_norm": 15.497636795043945,
      "learning_rate": 9.217897091722596e-06,
      "loss": 0.1754,
      "step": 12050
    },
    {
      "epoch": 27.069351230425056,
      "grad_norm": 26.197429656982422,
      "learning_rate": 9.173154362416107e-06,
      "loss": 0.1566,
      "step": 12100
    },
    {
      "epoch": 27.181208053691275,
      "grad_norm": 16.69260597229004,
      "learning_rate": 9.12841163310962e-06,
      "loss": 0.168,
      "step": 12150
    },
    {
      "epoch": 27.293064876957494,
      "grad_norm": 21.498811721801758,
      "learning_rate": 9.083668903803132e-06,
      "loss": 0.1662,
      "step": 12200
    },
    {
      "epoch": 27.404921700223714,
      "grad_norm": 19.863840103149414,
      "learning_rate": 9.038926174496644e-06,
      "loss": 0.157,
      "step": 12250
    },
    {
      "epoch": 27.516778523489933,
      "grad_norm": 17.655900955200195,
      "learning_rate": 8.994183445190157e-06,
      "loss": 0.1618,
      "step": 12300
    },
    {
      "epoch": 27.628635346756152,
      "grad_norm": 403.6722717285156,
      "learning_rate": 8.949440715883669e-06,
      "loss": 0.1657,
      "step": 12350
    },
    {
      "epoch": 27.74049217002237,
      "grad_norm": 15.591458320617676,
      "learning_rate": 8.904697986577182e-06,
      "loss": 0.1575,
      "step": 12400
    },
    {
      "epoch": 27.85234899328859,
      "grad_norm": 23.084238052368164,
      "learning_rate": 8.859955257270694e-06,
      "loss": 0.1734,
      "step": 12450
    },
    {
      "epoch": 27.96420581655481,
      "grad_norm": 12.84077262878418,
      "learning_rate": 8.815212527964206e-06,
      "loss": 0.1813,
      "step": 12500
    },
    {
      "epoch": 28.07606263982103,
      "grad_norm": 11.254690170288086,
      "learning_rate": 8.770469798657719e-06,
      "loss": 0.1767,
      "step": 12550
    },
    {
      "epoch": 28.187919463087248,
      "grad_norm": 19.5108585357666,
      "learning_rate": 8.725727069351232e-06,
      "loss": 0.145,
      "step": 12600
    },
    {
      "epoch": 28.299776286353467,
      "grad_norm": 38.838809967041016,
      "learning_rate": 8.680984340044743e-06,
      "loss": 0.1674,
      "step": 12650
    },
    {
      "epoch": 28.411633109619686,
      "grad_norm": 24.736146926879883,
      "learning_rate": 8.636241610738256e-06,
      "loss": 0.1557,
      "step": 12700
    },
    {
      "epoch": 28.523489932885905,
      "grad_norm": 17.258502960205078,
      "learning_rate": 8.591498881431769e-06,
      "loss": 0.1622,
      "step": 12750
    },
    {
      "epoch": 28.635346756152124,
      "grad_norm": 16.250076293945312,
      "learning_rate": 8.54675615212528e-06,
      "loss": 0.1617,
      "step": 12800
    },
    {
      "epoch": 28.747203579418343,
      "grad_norm": 13.93774127960205,
      "learning_rate": 8.502013422818793e-06,
      "loss": 0.1611,
      "step": 12850
    },
    {
      "epoch": 28.859060402684563,
      "grad_norm": 20.484228134155273,
      "learning_rate": 8.457270693512306e-06,
      "loss": 0.157,
      "step": 12900
    },
    {
      "epoch": 28.97091722595078,
      "grad_norm": 19.0781192779541,
      "learning_rate": 8.412527964205819e-06,
      "loss": 0.1657,
      "step": 12950
    },
    {
      "epoch": 29.082774049217,
      "grad_norm": 19.964412689208984,
      "learning_rate": 8.36778523489933e-06,
      "loss": 0.1682,
      "step": 13000
    },
    {
      "epoch": 29.19463087248322,
      "grad_norm": 16.539424896240234,
      "learning_rate": 8.323042505592843e-06,
      "loss": 0.1692,
      "step": 13050
    },
    {
      "epoch": 29.30648769574944,
      "grad_norm": 30.594648361206055,
      "learning_rate": 8.278299776286354e-06,
      "loss": 0.1513,
      "step": 13100
    },
    {
      "epoch": 29.41834451901566,
      "grad_norm": 27.73572540283203,
      "learning_rate": 8.233557046979867e-06,
      "loss": 0.1662,
      "step": 13150
    },
    {
      "epoch": 29.530201342281877,
      "grad_norm": 27.639747619628906,
      "learning_rate": 8.18881431767338e-06,
      "loss": 0.155,
      "step": 13200
    },
    {
      "epoch": 29.6420581655481,
      "grad_norm": 15.028164863586426,
      "learning_rate": 8.144071588366891e-06,
      "loss": 0.149,
      "step": 13250
    },
    {
      "epoch": 29.75391498881432,
      "grad_norm": 24.26503562927246,
      "learning_rate": 8.099328859060404e-06,
      "loss": 0.1543,
      "step": 13300
    },
    {
      "epoch": 29.86577181208054,
      "grad_norm": 29.7891788482666,
      "learning_rate": 8.054586129753915e-06,
      "loss": 0.1413,
      "step": 13350
    },
    {
      "epoch": 29.977628635346758,
      "grad_norm": 19.090003967285156,
      "learning_rate": 8.009843400447428e-06,
      "loss": 0.1565,
      "step": 13400
    },
    {
      "epoch": 30.089485458612977,
      "grad_norm": 21.77358055114746,
      "learning_rate": 7.965100671140941e-06,
      "loss": 0.1613,
      "step": 13450
    },
    {
      "epoch": 30.201342281879196,
      "grad_norm": 1795.65869140625,
      "learning_rate": 7.920357941834452e-06,
      "loss": 0.1486,
      "step": 13500
    },
    {
      "epoch": 30.313199105145415,
      "grad_norm": 19.762399673461914,
      "learning_rate": 7.875615212527965e-06,
      "loss": 0.1456,
      "step": 13550
    },
    {
      "epoch": 30.425055928411634,
      "grad_norm": 22.311372756958008,
      "learning_rate": 7.830872483221476e-06,
      "loss": 0.1519,
      "step": 13600
    },
    {
      "epoch": 30.536912751677853,
      "grad_norm": 20.708620071411133,
      "learning_rate": 7.78612975391499e-06,
      "loss": 0.1495,
      "step": 13650
    },
    {
      "epoch": 30.648769574944073,
      "grad_norm": 23.039987564086914,
      "learning_rate": 7.741387024608502e-06,
      "loss": 0.1525,
      "step": 13700
    },
    {
      "epoch": 30.76062639821029,
      "grad_norm": 19.454883575439453,
      "learning_rate": 7.696644295302013e-06,
      "loss": 0.1583,
      "step": 13750
    },
    {
      "epoch": 30.87248322147651,
      "grad_norm": 29.72968292236328,
      "learning_rate": 7.651901565995526e-06,
      "loss": 0.1505,
      "step": 13800
    },
    {
      "epoch": 30.98434004474273,
      "grad_norm": 27.202659606933594,
      "learning_rate": 7.607158836689039e-06,
      "loss": 0.1529,
      "step": 13850
    },
    {
      "epoch": 31.09619686800895,
      "grad_norm": 29.370527267456055,
      "learning_rate": 7.5624161073825505e-06,
      "loss": 0.1558,
      "step": 13900
    },
    {
      "epoch": 31.20805369127517,
      "grad_norm": 46.118350982666016,
      "learning_rate": 7.517673378076063e-06,
      "loss": 0.1567,
      "step": 13950
    },
    {
      "epoch": 31.319910514541387,
      "grad_norm": 11.725095748901367,
      "learning_rate": 7.4729306487695754e-06,
      "loss": 0.1522,
      "step": 14000
    },
    {
      "epoch": 31.431767337807607,
      "grad_norm": 26.406984329223633,
      "learning_rate": 7.4281879194630875e-06,
      "loss": 0.1534,
      "step": 14050
    },
    {
      "epoch": 31.543624161073826,
      "grad_norm": 18.919960021972656,
      "learning_rate": 7.3834451901566004e-06,
      "loss": 0.1542,
      "step": 14100
    },
    {
      "epoch": 31.655480984340045,
      "grad_norm": 11.363759994506836,
      "learning_rate": 7.3387024608501125e-06,
      "loss": 0.1475,
      "step": 14150
    },
    {
      "epoch": 31.767337807606264,
      "grad_norm": 103.8605728149414,
      "learning_rate": 7.2939597315436254e-06,
      "loss": 0.1511,
      "step": 14200
    },
    {
      "epoch": 31.879194630872483,
      "grad_norm": 22.609142303466797,
      "learning_rate": 7.249217002237137e-06,
      "loss": 0.1632,
      "step": 14250
    },
    {
      "epoch": 31.991051454138702,
      "grad_norm": 25.330101013183594,
      "learning_rate": 7.2044742729306496e-06,
      "loss": 0.1408,
      "step": 14300
    },
    {
      "epoch": 32.10290827740492,
      "grad_norm": 18.509727478027344,
      "learning_rate": 7.1597315436241625e-06,
      "loss": 0.1324,
      "step": 14350
    },
    {
      "epoch": 32.214765100671144,
      "grad_norm": 16.593055725097656,
      "learning_rate": 7.114988814317674e-06,
      "loss": 0.1551,
      "step": 14400
    },
    {
      "epoch": 32.32662192393736,
      "grad_norm": 16.658044815063477,
      "learning_rate": 7.070246085011187e-06,
      "loss": 0.1486,
      "step": 14450
    },
    {
      "epoch": 32.43847874720358,
      "grad_norm": 39.62859344482422,
      "learning_rate": 7.025503355704698e-06,
      "loss": 0.1521,
      "step": 14500
    },
    {
      "epoch": 32.5503355704698,
      "grad_norm": 19.22323226928711,
      "learning_rate": 6.980760626398211e-06,
      "loss": 0.1687,
      "step": 14550
    },
    {
      "epoch": 32.66219239373602,
      "grad_norm": 26.46348762512207,
      "learning_rate": 6.936017897091724e-06,
      "loss": 0.1537,
      "step": 14600
    },
    {
      "epoch": 32.774049217002236,
      "grad_norm": 15.034067153930664,
      "learning_rate": 6.891275167785235e-06,
      "loss": 0.1474,
      "step": 14650
    },
    {
      "epoch": 32.88590604026846,
      "grad_norm": 18.49684715270996,
      "learning_rate": 6.846532438478748e-06,
      "loss": 0.1479,
      "step": 14700
    },
    {
      "epoch": 32.997762863534675,
      "grad_norm": 38.04959487915039,
      "learning_rate": 6.80178970917226e-06,
      "loss": 0.1525,
      "step": 14750
    },
    {
      "epoch": 33.1096196868009,
      "grad_norm": 25.42348289489746,
      "learning_rate": 6.757046979865772e-06,
      "loss": 0.1524,
      "step": 14800
    },
    {
      "epoch": 33.22147651006711,
      "grad_norm": 26.540531158447266,
      "learning_rate": 6.712304250559285e-06,
      "loss": 0.1448,
      "step": 14850
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 21.27132797241211,
      "learning_rate": 6.667561521252797e-06,
      "loss": 0.1607,
      "step": 14900
    },
    {
      "epoch": 33.44519015659955,
      "grad_norm": 65.48155212402344,
      "learning_rate": 6.622818791946309e-06,
      "loss": 0.1453,
      "step": 14950
    },
    {
      "epoch": 33.557046979865774,
      "grad_norm": 26.054841995239258,
      "learning_rate": 6.578076062639821e-06,
      "loss": 0.1492,
      "step": 15000
    },
    {
      "epoch": 33.66890380313199,
      "grad_norm": 20.305482864379883,
      "learning_rate": 6.533333333333334e-06,
      "loss": 0.1497,
      "step": 15050
    },
    {
      "epoch": 33.78076062639821,
      "grad_norm": 229.06138610839844,
      "learning_rate": 6.488590604026846e-06,
      "loss": 0.1489,
      "step": 15100
    },
    {
      "epoch": 33.89261744966443,
      "grad_norm": 22.04414939880371,
      "learning_rate": 6.443847874720358e-06,
      "loss": 0.1494,
      "step": 15150
    },
    {
      "epoch": 34.00447427293065,
      "grad_norm": 65.59403228759766,
      "learning_rate": 6.399105145413871e-06,
      "loss": 0.1438,
      "step": 15200
    },
    {
      "epoch": 34.116331096196866,
      "grad_norm": 17.300798416137695,
      "learning_rate": 6.354362416107383e-06,
      "loss": 0.1506,
      "step": 15250
    },
    {
      "epoch": 34.22818791946309,
      "grad_norm": 25.116422653198242,
      "learning_rate": 6.309619686800895e-06,
      "loss": 0.1493,
      "step": 15300
    },
    {
      "epoch": 34.340044742729305,
      "grad_norm": 22.379785537719727,
      "learning_rate": 6.264876957494408e-06,
      "loss": 0.1415,
      "step": 15350
    },
    {
      "epoch": 34.45190156599553,
      "grad_norm": 14.227214813232422,
      "learning_rate": 6.220134228187919e-06,
      "loss": 0.1501,
      "step": 15400
    },
    {
      "epoch": 34.56375838926174,
      "grad_norm": 24.339387893676758,
      "learning_rate": 6.175391498881432e-06,
      "loss": 0.1347,
      "step": 15450
    },
    {
      "epoch": 34.675615212527966,
      "grad_norm": 24.612871170043945,
      "learning_rate": 6.130648769574945e-06,
      "loss": 0.1509,
      "step": 15500
    },
    {
      "epoch": 34.78747203579418,
      "grad_norm": 30.94973373413086,
      "learning_rate": 6.0859060402684564e-06,
      "loss": 0.1354,
      "step": 15550
    },
    {
      "epoch": 34.899328859060404,
      "grad_norm": 14.626932144165039,
      "learning_rate": 6.041163310961969e-06,
      "loss": 0.1379,
      "step": 15600
    },
    {
      "epoch": 35.01118568232662,
      "grad_norm": 15.62807559967041,
      "learning_rate": 5.996420581655481e-06,
      "loss": 0.1523,
      "step": 15650
    },
    {
      "epoch": 35.12304250559284,
      "grad_norm": 19.357072830200195,
      "learning_rate": 5.9516778523489935e-06,
      "loss": 0.1369,
      "step": 15700
    },
    {
      "epoch": 35.23489932885906,
      "grad_norm": 19.856117248535156,
      "learning_rate": 5.906935123042506e-06,
      "loss": 0.1488,
      "step": 15750
    },
    {
      "epoch": 35.34675615212528,
      "grad_norm": 25.956016540527344,
      "learning_rate": 5.8621923937360185e-06,
      "loss": 0.1428,
      "step": 15800
    },
    {
      "epoch": 35.458612975391496,
      "grad_norm": 14.285713195800781,
      "learning_rate": 5.8174496644295306e-06,
      "loss": 0.1306,
      "step": 15850
    },
    {
      "epoch": 35.57046979865772,
      "grad_norm": 14.47927188873291,
      "learning_rate": 5.772706935123043e-06,
      "loss": 0.1466,
      "step": 15900
    },
    {
      "epoch": 35.682326621923934,
      "grad_norm": 26.420454025268555,
      "learning_rate": 5.7279642058165555e-06,
      "loss": 0.1279,
      "step": 15950
    },
    {
      "epoch": 35.79418344519016,
      "grad_norm": 13.22993278503418,
      "learning_rate": 5.683221476510068e-06,
      "loss": 0.1391,
      "step": 16000
    },
    {
      "epoch": 35.90604026845637,
      "grad_norm": 20.775659561157227,
      "learning_rate": 5.63847874720358e-06,
      "loss": 0.1336,
      "step": 16050
    },
    {
      "epoch": 36.017897091722595,
      "grad_norm": 44.73728942871094,
      "learning_rate": 5.593736017897093e-06,
      "loss": 0.1338,
      "step": 16100
    },
    {
      "epoch": 36.12975391498881,
      "grad_norm": 24.79612159729004,
      "learning_rate": 5.548993288590604e-06,
      "loss": 0.129,
      "step": 16150
    },
    {
      "epoch": 36.241610738255034,
      "grad_norm": 15.258400917053223,
      "learning_rate": 5.504250559284117e-06,
      "loss": 0.1337,
      "step": 16200
    },
    {
      "epoch": 36.353467561521256,
      "grad_norm": 13.570374488830566,
      "learning_rate": 5.45950782997763e-06,
      "loss": 0.151,
      "step": 16250
    },
    {
      "epoch": 36.46532438478747,
      "grad_norm": 17.458017349243164,
      "learning_rate": 5.414765100671141e-06,
      "loss": 0.1327,
      "step": 16300
    },
    {
      "epoch": 36.577181208053695,
      "grad_norm": 20.59542465209961,
      "learning_rate": 5.370022371364654e-06,
      "loss": 0.1391,
      "step": 16350
    },
    {
      "epoch": 36.68903803131991,
      "grad_norm": 23.082529067993164,
      "learning_rate": 5.325279642058167e-06,
      "loss": 0.1326,
      "step": 16400
    },
    {
      "epoch": 36.80089485458613,
      "grad_norm": 14.710920333862305,
      "learning_rate": 5.280536912751678e-06,
      "loss": 0.1326,
      "step": 16450
    },
    {
      "epoch": 36.91275167785235,
      "grad_norm": 27.254047393798828,
      "learning_rate": 5.235794183445191e-06,
      "loss": 0.1325,
      "step": 16500
    },
    {
      "epoch": 37.02460850111857,
      "grad_norm": 17.83852767944336,
      "learning_rate": 5.191051454138702e-06,
      "loss": 0.1278,
      "step": 16550
    },
    {
      "epoch": 37.13646532438479,
      "grad_norm": 67.57838439941406,
      "learning_rate": 5.146308724832215e-06,
      "loss": 0.1258,
      "step": 16600
    },
    {
      "epoch": 37.24832214765101,
      "grad_norm": 29.435806274414062,
      "learning_rate": 5.101565995525728e-06,
      "loss": 0.1333,
      "step": 16650
    },
    {
      "epoch": 37.360178970917225,
      "grad_norm": 16.860559463500977,
      "learning_rate": 5.05682326621924e-06,
      "loss": 0.1361,
      "step": 16700
    },
    {
      "epoch": 37.47203579418345,
      "grad_norm": 15.794093132019043,
      "learning_rate": 5.012080536912752e-06,
      "loss": 0.1374,
      "step": 16750
    },
    {
      "epoch": 37.58389261744966,
      "grad_norm": 20.13224220275879,
      "learning_rate": 4.967337807606264e-06,
      "loss": 0.1366,
      "step": 16800
    },
    {
      "epoch": 37.695749440715886,
      "grad_norm": 29.04473876953125,
      "learning_rate": 4.922595078299777e-06,
      "loss": 0.1327,
      "step": 16850
    },
    {
      "epoch": 37.8076062639821,
      "grad_norm": 28.447208404541016,
      "learning_rate": 4.877852348993289e-06,
      "loss": 0.1312,
      "step": 16900
    },
    {
      "epoch": 37.919463087248324,
      "grad_norm": 26.99588394165039,
      "learning_rate": 4.833109619686801e-06,
      "loss": 0.1342,
      "step": 16950
    },
    {
      "epoch": 38.03131991051454,
      "grad_norm": 16.19617462158203,
      "learning_rate": 4.788366890380314e-06,
      "loss": 0.1329,
      "step": 17000
    },
    {
      "epoch": 38.14317673378076,
      "grad_norm": 19.232717514038086,
      "learning_rate": 4.743624161073826e-06,
      "loss": 0.1273,
      "step": 17050
    },
    {
      "epoch": 38.25503355704698,
      "grad_norm": 13.836492538452148,
      "learning_rate": 4.698881431767338e-06,
      "loss": 0.1311,
      "step": 17100
    },
    {
      "epoch": 38.3668903803132,
      "grad_norm": 14.067038536071777,
      "learning_rate": 4.65413870246085e-06,
      "loss": 0.1303,
      "step": 17150
    },
    {
      "epoch": 38.47874720357942,
      "grad_norm": 17.419424057006836,
      "learning_rate": 4.609395973154363e-06,
      "loss": 0.1393,
      "step": 17200
    },
    {
      "epoch": 38.59060402684564,
      "grad_norm": 14.508977890014648,
      "learning_rate": 4.564653243847875e-06,
      "loss": 0.1216,
      "step": 17250
    },
    {
      "epoch": 38.702460850111855,
      "grad_norm": 562.0117797851562,
      "learning_rate": 4.519910514541387e-06,
      "loss": 0.1248,
      "step": 17300
    },
    {
      "epoch": 38.81431767337808,
      "grad_norm": 24.66980743408203,
      "learning_rate": 4.4751677852348995e-06,
      "loss": 0.1287,
      "step": 17350
    },
    {
      "epoch": 38.92617449664429,
      "grad_norm": 12.110504150390625,
      "learning_rate": 4.4304250559284115e-06,
      "loss": 0.1316,
      "step": 17400
    },
    {
      "epoch": 39.038031319910516,
      "grad_norm": 35.986915588378906,
      "learning_rate": 4.3856823266219245e-06,
      "loss": 0.1277,
      "step": 17450
    },
    {
      "epoch": 39.14988814317673,
      "grad_norm": 17.999114990234375,
      "learning_rate": 4.3409395973154365e-06,
      "loss": 0.1261,
      "step": 17500
    },
    {
      "epoch": 39.261744966442954,
      "grad_norm": 525.751953125,
      "learning_rate": 4.296196868008949e-06,
      "loss": 0.1288,
      "step": 17550
    },
    {
      "epoch": 39.37360178970917,
      "grad_norm": 22.8410587310791,
      "learning_rate": 4.251454138702461e-06,
      "loss": 0.1329,
      "step": 17600
    },
    {
      "epoch": 39.48545861297539,
      "grad_norm": 16.172607421875,
      "learning_rate": 4.206711409395974e-06,
      "loss": 0.1238,
      "step": 17650
    },
    {
      "epoch": 39.59731543624161,
      "grad_norm": 133.26356506347656,
      "learning_rate": 4.161968680089486e-06,
      "loss": 0.1248,
      "step": 17700
    },
    {
      "epoch": 39.70917225950783,
      "grad_norm": 16.206844329833984,
      "learning_rate": 4.117225950782998e-06,
      "loss": 0.1245,
      "step": 17750
    },
    {
      "epoch": 39.82102908277405,
      "grad_norm": 25.74180030822754,
      "learning_rate": 4.072483221476511e-06,
      "loss": 0.1211,
      "step": 17800
    },
    {
      "epoch": 39.93288590604027,
      "grad_norm": 115.36943817138672,
      "learning_rate": 4.027740492170023e-06,
      "loss": 0.1251,
      "step": 17850
    },
    {
      "epoch": 40.044742729306485,
      "grad_norm": 32.57194519042969,
      "learning_rate": 3.982997762863536e-06,
      "loss": 0.1394,
      "step": 17900
    },
    {
      "epoch": 40.15659955257271,
      "grad_norm": 24.614023208618164,
      "learning_rate": 3.938255033557048e-06,
      "loss": 0.1248,
      "step": 17950
    },
    {
      "epoch": 40.26845637583892,
      "grad_norm": 207.98355102539062,
      "learning_rate": 3.89351230425056e-06,
      "loss": 0.1182,
      "step": 18000
    },
    {
      "epoch": 40.380313199105146,
      "grad_norm": 1040.2607421875,
      "learning_rate": 3.848769574944072e-06,
      "loss": 0.1296,
      "step": 18050
    },
    {
      "epoch": 40.49217002237136,
      "grad_norm": 15.12655258178711,
      "learning_rate": 3.804026845637584e-06,
      "loss": 0.1254,
      "step": 18100
    },
    {
      "epoch": 40.604026845637584,
      "grad_norm": 20.86418342590332,
      "learning_rate": 3.759284116331097e-06,
      "loss": 0.1321,
      "step": 18150
    },
    {
      "epoch": 40.7158836689038,
      "grad_norm": 27.973844528198242,
      "learning_rate": 3.714541387024609e-06,
      "loss": 0.1197,
      "step": 18200
    },
    {
      "epoch": 40.82774049217002,
      "grad_norm": 24.413877487182617,
      "learning_rate": 3.669798657718121e-06,
      "loss": 0.1231,
      "step": 18250
    },
    {
      "epoch": 40.939597315436245,
      "grad_norm": 15.159890174865723,
      "learning_rate": 3.625055928411633e-06,
      "loss": 0.1313,
      "step": 18300
    },
    {
      "epoch": 41.05145413870246,
      "grad_norm": 13.080474853515625,
      "learning_rate": 3.580313199105146e-06,
      "loss": 0.1261,
      "step": 18350
    },
    {
      "epoch": 41.16331096196868,
      "grad_norm": 25.409090042114258,
      "learning_rate": 3.535570469798658e-06,
      "loss": 0.1182,
      "step": 18400
    },
    {
      "epoch": 41.2751677852349,
      "grad_norm": 18.177494049072266,
      "learning_rate": 3.49082774049217e-06,
      "loss": 0.131,
      "step": 18450
    },
    {
      "epoch": 41.38702460850112,
      "grad_norm": 19.388643264770508,
      "learning_rate": 3.4460850111856826e-06,
      "loss": 0.1276,
      "step": 18500
    },
    {
      "epoch": 41.49888143176734,
      "grad_norm": 32.253021240234375,
      "learning_rate": 3.4013422818791947e-06,
      "loss": 0.1242,
      "step": 18550
    },
    {
      "epoch": 41.61073825503356,
      "grad_norm": 22.54273223876953,
      "learning_rate": 3.3565995525727076e-06,
      "loss": 0.125,
      "step": 18600
    },
    {
      "epoch": 41.722595078299776,
      "grad_norm": 21.069908142089844,
      "learning_rate": 3.3118568232662197e-06,
      "loss": 0.1184,
      "step": 18650
    },
    {
      "epoch": 41.834451901566,
      "grad_norm": 17.84953498840332,
      "learning_rate": 3.2671140939597317e-06,
      "loss": 0.1247,
      "step": 18700
    },
    {
      "epoch": 41.946308724832214,
      "grad_norm": 20.119152069091797,
      "learning_rate": 3.222371364653244e-06,
      "loss": 0.1205,
      "step": 18750
    },
    {
      "epoch": 42.05816554809844,
      "grad_norm": 18.767807006835938,
      "learning_rate": 3.1776286353467563e-06,
      "loss": 0.1258,
      "step": 18800
    },
    {
      "epoch": 42.17002237136465,
      "grad_norm": 149.0808563232422,
      "learning_rate": 3.132885906040269e-06,
      "loss": 0.1172,
      "step": 18850
    },
    {
      "epoch": 42.281879194630875,
      "grad_norm": 15.990947723388672,
      "learning_rate": 3.088143176733781e-06,
      "loss": 0.1136,
      "step": 18900
    },
    {
      "epoch": 42.39373601789709,
      "grad_norm": 9.630525588989258,
      "learning_rate": 3.0434004474272934e-06,
      "loss": 0.1267,
      "step": 18950
    },
    {
      "epoch": 42.50559284116331,
      "grad_norm": 26.01500701904297,
      "learning_rate": 2.9986577181208054e-06,
      "loss": 0.1183,
      "step": 19000
    },
    {
      "epoch": 42.61744966442953,
      "grad_norm": 26.8444881439209,
      "learning_rate": 2.953914988814318e-06,
      "loss": 0.1207,
      "step": 19050
    },
    {
      "epoch": 42.72930648769575,
      "grad_norm": 21.117956161499023,
      "learning_rate": 2.9091722595078304e-06,
      "loss": 0.1354,
      "step": 19100
    },
    {
      "epoch": 42.84116331096197,
      "grad_norm": 19.381732940673828,
      "learning_rate": 2.8644295302013425e-06,
      "loss": 0.1219,
      "step": 19150
    },
    {
      "epoch": 42.95302013422819,
      "grad_norm": 25.253616333007812,
      "learning_rate": 2.8196868008948546e-06,
      "loss": 0.1101,
      "step": 19200
    },
    {
      "epoch": 43.064876957494405,
      "grad_norm": 12.28507137298584,
      "learning_rate": 2.774944071588367e-06,
      "loss": 0.1212,
      "step": 19250
    },
    {
      "epoch": 43.17673378076063,
      "grad_norm": 26.298852920532227,
      "learning_rate": 2.7302013422818796e-06,
      "loss": 0.1221,
      "step": 19300
    },
    {
      "epoch": 43.288590604026844,
      "grad_norm": 18.863901138305664,
      "learning_rate": 2.6854586129753916e-06,
      "loss": 0.1223,
      "step": 19350
    },
    {
      "epoch": 43.40044742729307,
      "grad_norm": 16.88353157043457,
      "learning_rate": 2.640715883668904e-06,
      "loss": 0.1222,
      "step": 19400
    },
    {
      "epoch": 43.51230425055928,
      "grad_norm": 20.532909393310547,
      "learning_rate": 2.595973154362416e-06,
      "loss": 0.115,
      "step": 19450
    },
    {
      "epoch": 43.624161073825505,
      "grad_norm": 18.67132568359375,
      "learning_rate": 2.5512304250559287e-06,
      "loss": 0.1244,
      "step": 19500
    },
    {
      "epoch": 43.73601789709172,
      "grad_norm": 21.879867553710938,
      "learning_rate": 2.506487695749441e-06,
      "loss": 0.1271,
      "step": 19550
    },
    {
      "epoch": 43.84787472035794,
      "grad_norm": 36.34318542480469,
      "learning_rate": 2.4617449664429533e-06,
      "loss": 0.1138,
      "step": 19600
    },
    {
      "epoch": 43.95973154362416,
      "grad_norm": 13.398548126220703,
      "learning_rate": 2.4170022371364653e-06,
      "loss": 0.1091,
      "step": 19650
    },
    {
      "epoch": 44.07158836689038,
      "grad_norm": 17.423433303833008,
      "learning_rate": 2.372259507829978e-06,
      "loss": 0.1217,
      "step": 19700
    },
    {
      "epoch": 44.1834451901566,
      "grad_norm": 627.7169799804688,
      "learning_rate": 2.32751677852349e-06,
      "loss": 0.1228,
      "step": 19750
    },
    {
      "epoch": 44.29530201342282,
      "grad_norm": 454.0099182128906,
      "learning_rate": 2.2827740492170024e-06,
      "loss": 0.1215,
      "step": 19800
    },
    {
      "epoch": 44.407158836689035,
      "grad_norm": 21.93001365661621,
      "learning_rate": 2.238031319910515e-06,
      "loss": 0.1193,
      "step": 19850
    },
    {
      "epoch": 44.51901565995526,
      "grad_norm": 17.354808807373047,
      "learning_rate": 2.193288590604027e-06,
      "loss": 0.1153,
      "step": 19900
    },
    {
      "epoch": 44.630872483221474,
      "grad_norm": 27.52999496459961,
      "learning_rate": 2.1485458612975395e-06,
      "loss": 0.1117,
      "step": 19950
    },
    {
      "epoch": 44.742729306487696,
      "grad_norm": 16.629980087280273,
      "learning_rate": 2.1038031319910515e-06,
      "loss": 0.1117,
      "step": 20000
    },
    {
      "epoch": 44.85458612975391,
      "grad_norm": 31.789934158325195,
      "learning_rate": 2.059060402684564e-06,
      "loss": 0.1166,
      "step": 20050
    },
    {
      "epoch": 44.966442953020135,
      "grad_norm": 21.450437545776367,
      "learning_rate": 2.014317673378076e-06,
      "loss": 0.1135,
      "step": 20100
    },
    {
      "epoch": 45.07829977628635,
      "grad_norm": 115.04971313476562,
      "learning_rate": 1.9695749440715886e-06,
      "loss": 0.1086,
      "step": 20150
    },
    {
      "epoch": 45.19015659955257,
      "grad_norm": 21.208581924438477,
      "learning_rate": 1.9248322147651007e-06,
      "loss": 0.1161,
      "step": 20200
    },
    {
      "epoch": 45.30201342281879,
      "grad_norm": 17.881196975708008,
      "learning_rate": 1.8800894854586132e-06,
      "loss": 0.122,
      "step": 20250
    },
    {
      "epoch": 45.41387024608501,
      "grad_norm": 23.125516891479492,
      "learning_rate": 1.8353467561521254e-06,
      "loss": 0.1093,
      "step": 20300
    },
    {
      "epoch": 45.525727069351234,
      "grad_norm": 17.494802474975586,
      "learning_rate": 1.7906040268456375e-06,
      "loss": 0.1238,
      "step": 20350
    },
    {
      "epoch": 45.63758389261745,
      "grad_norm": 16.328039169311523,
      "learning_rate": 1.74586129753915e-06,
      "loss": 0.1057,
      "step": 20400
    },
    {
      "epoch": 45.74944071588367,
      "grad_norm": 16.21575927734375,
      "learning_rate": 1.7011185682326623e-06,
      "loss": 0.1167,
      "step": 20450
    },
    {
      "epoch": 45.86129753914989,
      "grad_norm": 26.598270416259766,
      "learning_rate": 1.6563758389261748e-06,
      "loss": 0.1083,
      "step": 20500
    },
    {
      "epoch": 45.97315436241611,
      "grad_norm": 20.425228118896484,
      "learning_rate": 1.6116331096196869e-06,
      "loss": 0.1084,
      "step": 20550
    },
    {
      "epoch": 46.085011185682326,
      "grad_norm": 17.075822830200195,
      "learning_rate": 1.5668903803131994e-06,
      "loss": 0.1162,
      "step": 20600
    },
    {
      "epoch": 46.19686800894855,
      "grad_norm": 26.532793045043945,
      "learning_rate": 1.5221476510067114e-06,
      "loss": 0.113,
      "step": 20650
    },
    {
      "epoch": 46.308724832214764,
      "grad_norm": 27.045452117919922,
      "learning_rate": 1.4774049217002237e-06,
      "loss": 0.1036,
      "step": 20700
    },
    {
      "epoch": 46.42058165548099,
      "grad_norm": 17.83247947692871,
      "learning_rate": 1.4326621923937362e-06,
      "loss": 0.122,
      "step": 20750
    },
    {
      "epoch": 46.5324384787472,
      "grad_norm": 19.38603401184082,
      "learning_rate": 1.3879194630872483e-06,
      "loss": 0.124,
      "step": 20800
    },
    {
      "epoch": 46.644295302013425,
      "grad_norm": 24.240427017211914,
      "learning_rate": 1.3431767337807608e-06,
      "loss": 0.1165,
      "step": 20850
    },
    {
      "epoch": 46.75615212527964,
      "grad_norm": 24.445920944213867,
      "learning_rate": 1.298434004474273e-06,
      "loss": 0.1142,
      "step": 20900
    },
    {
      "epoch": 46.868008948545864,
      "grad_norm": 17.96455955505371,
      "learning_rate": 1.2536912751677853e-06,
      "loss": 0.1163,
      "step": 20950
    },
    {
      "epoch": 46.97986577181208,
      "grad_norm": 111.82260131835938,
      "learning_rate": 1.2089485458612976e-06,
      "loss": 0.1125,
      "step": 21000
    },
    {
      "epoch": 47.0917225950783,
      "grad_norm": 11.280172348022461,
      "learning_rate": 1.16420581655481e-06,
      "loss": 0.1112,
      "step": 21050
    },
    {
      "epoch": 47.20357941834452,
      "grad_norm": 17.552959442138672,
      "learning_rate": 1.1194630872483222e-06,
      "loss": 0.1165,
      "step": 21100
    },
    {
      "epoch": 47.31543624161074,
      "grad_norm": 41.633243560791016,
      "learning_rate": 1.0747203579418347e-06,
      "loss": 0.1091,
      "step": 21150
    },
    {
      "epoch": 47.427293064876956,
      "grad_norm": 152.72216796875,
      "learning_rate": 1.029977628635347e-06,
      "loss": 0.1134,
      "step": 21200
    },
    {
      "epoch": 47.53914988814318,
      "grad_norm": 26.940059661865234,
      "learning_rate": 9.85234899328859e-07,
      "loss": 0.1171,
      "step": 21250
    },
    {
      "epoch": 47.651006711409394,
      "grad_norm": 25.117759704589844,
      "learning_rate": 9.404921700223714e-07,
      "loss": 0.1137,
      "step": 21300
    },
    {
      "epoch": 47.76286353467562,
      "grad_norm": 26.83030128479004,
      "learning_rate": 8.957494407158837e-07,
      "loss": 0.1096,
      "step": 21350
    },
    {
      "epoch": 47.87472035794183,
      "grad_norm": 19.49488067626953,
      "learning_rate": 8.51006711409396e-07,
      "loss": 0.1186,
      "step": 21400
    },
    {
      "epoch": 47.986577181208055,
      "grad_norm": 9.085994720458984,
      "learning_rate": 8.062639821029084e-07,
      "loss": 0.1049,
      "step": 21450
    },
    {
      "epoch": 48.09843400447427,
      "grad_norm": 30.314422607421875,
      "learning_rate": 7.615212527964207e-07,
      "loss": 0.1171,
      "step": 21500
    },
    {
      "epoch": 48.210290827740494,
      "grad_norm": 19.410099029541016,
      "learning_rate": 7.167785234899329e-07,
      "loss": 0.1043,
      "step": 21550
    },
    {
      "epoch": 48.32214765100671,
      "grad_norm": 19.906375885009766,
      "learning_rate": 6.720357941834453e-07,
      "loss": 0.1133,
      "step": 21600
    },
    {
      "epoch": 48.43400447427293,
      "grad_norm": 23.844633102416992,
      "learning_rate": 6.272930648769575e-07,
      "loss": 0.1161,
      "step": 21650
    },
    {
      "epoch": 48.54586129753915,
      "grad_norm": 14.021733283996582,
      "learning_rate": 5.825503355704699e-07,
      "loss": 0.108,
      "step": 21700
    },
    {
      "epoch": 48.65771812080537,
      "grad_norm": 36.99049377441406,
      "learning_rate": 5.378076062639821e-07,
      "loss": 0.1047,
      "step": 21750
    },
    {
      "epoch": 48.769574944071586,
      "grad_norm": 17.330162048339844,
      "learning_rate": 4.930648769574945e-07,
      "loss": 0.1189,
      "step": 21800
    },
    {
      "epoch": 48.88143176733781,
      "grad_norm": 12.104101181030273,
      "learning_rate": 4.4832214765100675e-07,
      "loss": 0.1144,
      "step": 21850
    },
    {
      "epoch": 48.993288590604024,
      "grad_norm": 21.008710861206055,
      "learning_rate": 4.035794183445191e-07,
      "loss": 0.1057,
      "step": 21900
    },
    {
      "epoch": 49.10514541387025,
      "grad_norm": 16.27073097229004,
      "learning_rate": 3.588366890380313e-07,
      "loss": 0.1141,
      "step": 21950
    },
    {
      "epoch": 49.21700223713646,
      "grad_norm": 30.134307861328125,
      "learning_rate": 3.1409395973154365e-07,
      "loss": 0.1046,
      "step": 22000
    },
    {
      "epoch": 49.328859060402685,
      "grad_norm": 18.85601234436035,
      "learning_rate": 2.6935123042505593e-07,
      "loss": 0.1079,
      "step": 22050
    },
    {
      "epoch": 49.4407158836689,
      "grad_norm": 13.491329193115234,
      "learning_rate": 2.2460850111856824e-07,
      "loss": 0.1128,
      "step": 22100
    },
    {
      "epoch": 49.55257270693512,
      "grad_norm": 14.56363296508789,
      "learning_rate": 1.7986577181208052e-07,
      "loss": 0.107,
      "step": 22150
    },
    {
      "epoch": 49.66442953020134,
      "grad_norm": 15.512772560119629,
      "learning_rate": 1.3512304250559286e-07,
      "loss": 0.1043,
      "step": 22200
    },
    {
      "epoch": 49.77628635346756,
      "grad_norm": 17.392459869384766,
      "learning_rate": 9.038031319910516e-08,
      "loss": 0.125,
      "step": 22250
    },
    {
      "epoch": 49.88814317673378,
      "grad_norm": 19.135087966918945,
      "learning_rate": 4.563758389261745e-08,
      "loss": 0.109,
      "step": 22300
    },
    {
      "epoch": 50.0,
      "grad_norm": 1067.930419921875,
      "learning_rate": 8.94854586129754e-10,
      "loss": 0.1123,
      "step": 22350
    }
  ],
  "logging_steps": 50,
  "max_steps": 22350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.459113188697604e+19,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
